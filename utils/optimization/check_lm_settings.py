# LM Studio Settings Checker and Optimizer
# This script checks current LM Studio settings and suggests optimizations

import requests
import json
import time
import sys

LM_STUDIO_BASE = "http://192.168.56.1:1234"

def check_connection():
    """Check if LM Studio is accessible"""
    try:
        response = requests.get(f"{LM_STUDIO_BASE}/v1/models", timeout=5)
        if response.status_code == 200:
            print("âœ… LM Studio connection successful")
            return True
        else:
            print(f"âŒ LM Studio returned status {response.status_code}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"âŒ Cannot connect to LM Studio: {e}")
        return False

def get_model_info():
    """Get current loaded model information"""
    try:
        response = requests.get(f"{LM_STUDIO_BASE}/v1/models", timeout=5)
        if response.status_code == 200:
            models = response.json()
            print(f"\nğŸ“Š Available Models ({len(models['data'])} total):")
            for model in models['data']:
                print(f"   ğŸ¤– {model['id']}")
            return models['data']
        return []
    except Exception as e:
        print(f"âŒ Error getting model info: {e}")
        return []

def test_completion_speed():
    """Test completion speed with current settings"""
    print(f"\nğŸ§ª Testing completion speed...")

    test_payload = {
        "model": "Qwen2-VL-7B-Instruct",
        "messages": [
            {"role": "user", "content": "Say 'Hello World' exactly."}
        ],
        "max_tokens": 10,
        "temperature": 0.1
    }

    try:
        start_time = time.time()
        response = requests.post(
            f"{LM_STUDIO_BASE}/v1/chat/completions",
            json=test_payload,
            timeout=30
        )
        end_time = time.time()

        if response.status_code == 200:
            duration = end_time - start_time
            result = response.json()
            content = result['choices'][0]['message']['content']
            print(f"âœ… Test completed in {duration:.2f} seconds")
            print(f"ğŸ“ Response: {content}")
            return duration
        else:
            print(f"âŒ Test failed with status {response.status_code}")
            print(f"Error: {response.text}")
            return None
    except Exception as e:
        print(f"âŒ Test error: {e}")
        return None

def test_vision_speed():
    """Test vision processing speed"""
    print(f"\nğŸ‘ï¸ Testing vision processing speed...")

    # Use a small test image (we'll encode a simple base64 image)
    test_image_b64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="

    test_payload = {
        "model": "Qwen2-VL-7B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What do you see?"},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{test_image_b64}"}}
                ]
            }
        ],
        "max_tokens": 50,
        "temperature": 0.1
    }

    try:
        start_time = time.time()
        response = requests.post(
            f"{LM_STUDIO_BASE}/v1/chat/completions",
            json=test_payload,
            timeout=30
        )
        end_time = time.time()

        if response.status_code == 200:
            duration = end_time - start_time
            result = response.json()
            content = result['choices'][0]['message']['content']
            print(f"âœ… Vision test completed in {duration:.2f} seconds")
            print(f"ğŸ“ Response: {content[:100]}...")
            return duration
        else:
            print(f"âŒ Vision test failed with status {response.status_code}")
            print(f"Error: {response.text}")
            return None
    except Exception as e:
        print(f"âŒ Vision test error: {e}")
        return None

def check_server_info():
    """Try to get server information"""
    print(f"\nğŸ” Checking server information...")

    # Try different endpoints that might give us info
    endpoints = [
        "/health",
        "/v1/completions",
        "/v1/embeddings",
        "/stats",
        "/info",
        "/status"
    ]

    for endpoint in endpoints:
        try:
            response = requests.get(f"{LM_STUDIO_BASE}{endpoint}", timeout=5)
            if response.status_code == 200:
                print(f"âœ… {endpoint}: Available")
                if endpoint == "/stats":
                    print(f"   Data: {response.text[:200]}...")
            elif response.status_code == 404:
                print(f"âŒ {endpoint}: Not found")
            else:
                print(f"âš ï¸  {endpoint}: Status {response.status_code}")
        except Exception as e:
            print(f"âŒ {endpoint}: Error - {e}")

def benchmark_different_settings():
    """Test with different parameter settings to find optimal"""
    print(f"\nâš¡ Benchmarking different settings...")

    test_configs = [
        {"max_tokens": 50, "temperature": 0.1, "name": "Fast & Focused"},
        {"max_tokens": 100, "temperature": 0.1, "name": "Medium & Focused"},
        {"max_tokens": 50, "temperature": 0.3, "name": "Fast & Creative"},
        {"max_tokens": 200, "temperature": 0.1, "name": "Long & Focused"},
    ]

    results = []

    for config in test_configs:
        print(f"\nğŸ§ª Testing: {config['name']}")

        test_payload = {
            "model": "Qwen2-VL-7B-Instruct",
            "messages": [
                {"role": "user", "content": "Describe the color blue in technical terms."}
            ],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }

        try:
            start_time = time.time()
            response = requests.post(
                f"{LM_STUDIO_BASE}/v1/chat/completions",
                json=test_payload,
                timeout=60
            )
            end_time = time.time()

            if response.status_code == 200:
                duration = end_time - start_time
                result = response.json()
                content = result['choices'][0]['message']['content']
                tokens_generated = len(content.split())

                print(f"   â±ï¸  Duration: {duration:.2f}s")
                print(f"   ğŸ“Š Tokens: ~{tokens_generated}")
                print(f"   ğŸš€ Speed: {tokens_generated/duration:.1f} tokens/sec")

                results.append({
                    "config": config['name'],
                    "duration": duration,
                    "tokens": tokens_generated,
                    "speed": tokens_generated/duration
                })
            else:
                print(f"   âŒ Failed: {response.status_code}")

        except Exception as e:
            print(f"   âŒ Error: {e}")

    if results:
        print(f"\nğŸ† Benchmark Results:")
        results.sort(key=lambda x: x['speed'], reverse=True)
        for i, result in enumerate(results, 1):
            print(f"   {i}. {result['config']}: {result['speed']:.1f} tokens/sec ({result['duration']:.2f}s)")

def generate_optimization_report():
    """Generate a comprehensive optimization report"""
    print(f"\nğŸ“‹ OPTIMIZATION RECOMMENDATIONS:")
    print(f"=" * 50)

    print(f"ğŸ”§ LM Studio GUI Settings to Check:")
    print(f"   1. Model Tab â†’ Advanced Settings:")
    print(f"      - Context Length: Set to 4096 (not 8192+)")
    print(f"      - GPU Layers: Set to -1 or 99 (use all GPU)")
    print(f"      - Batch Size: Increase to 512-1024")
    print(f"   2. Settings â†’ Hardware:")
    print(f"      - GPU Memory: Allocate 12-14GB")
    print(f"      - Keep CPU threads: 8-16")

    print(f"\nğŸ–¥ï¸ System Optimizations:")
    print(f"   1. Windows Power Plan: High Performance")
    print(f"   2. NVIDIA Control Panel:")
    print(f"      - Power management: Prefer maximum performance")
    print(f"      - CUDA GPUs: Use GeForce RTX 4080")

    print(f"\nğŸ“± Application Settings:")
    print(f"   1. Use max_tokens: 4096 (not 8192)")
    print(f"   2. Use temperature: 0.1-0.3 (not 0.5+)")
    print(f"   3. Reduce retry delays to 0.3s")

    print(f"\nğŸ¯ Expected Performance:")
    print(f"   - Target: 2-5 seconds per page")
    print(f"   - Current Vision Model: Qwen2-VL-7B-Instruct")
    print(f"   - GPU Usage: Should be 90%+ during processing")

def main():
    print("ğŸš€ LM Studio Settings Checker & Optimizer")
    print("=" * 50)

    # Basic connectivity check
    if not check_connection():
        print("\nâŒ Cannot proceed without LM Studio connection")
        sys.exit(1)

    # Get model information
    models = get_model_info()

    # Check server endpoints
    check_server_info()

    # Test current performance
    text_speed = test_completion_speed()
    vision_speed = test_vision_speed()

    # Benchmark different settings
    benchmark_different_settings()

    # Generate optimization report
    generate_optimization_report()

    print(f"\nâœ… Analysis complete!")
    if text_speed and vision_speed:
        print(f"ğŸ“Š Current Performance Summary:")
        print(f"   - Text completion: {text_speed:.2f}s")
        print(f"   - Vision processing: {vision_speed:.2f}s")

if __name__ == "__main__":
    main()
